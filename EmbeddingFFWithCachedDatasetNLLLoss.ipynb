{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Zuerst werden die benötigten Pakete installiert. Zusätzlich wird der Seed auf 0 Festgelegt um reproduzierbare Ergebnisse zu erhalten."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# für Deep Learning\n",
    "import torch\n",
    "\n",
    "# Für reproduzierbare Ergebnisse, den Seed festlegen\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import tensorboard\n",
    "\n",
    "# für Matrizen\n",
    "import numpy as np\n",
    "# Für Grafiken und Abbildungen\n",
    "import matplotlib.pyplot as plt\n",
    "# Zum Laden und vorverarbeiten von Dateien\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#Für Tabellen\n",
    "import pandas as pd\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "\n",
    "# Für Ladebalken\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Für Transformer Modelle\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertForMaskedLM,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d37605c9-e4a2-4441-81a6-d6f87f908fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from sentence-transformers) (1.24.3)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from sentence-transformers) (4.31.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from sentence-transformers) (0.16.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from nltk->sentence-transformers) (1.3.1)\n",
      "Requirement already satisfied: click in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from nltk->sentence-transformers) (8.1.6)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from torchvision->sentence-transformers) (10.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\phil\\pycharmprojects\\restaurantsprediction\\venv\\lib\\site-packages (1.3.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Installieren nötiger Pakete\n",
    "import sys\n",
    "!{sys.executable} -m pip install sentence-transformers\n",
    "!{sys.executable} -m pip install unidecode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb859084-17a0-4918-ab93-321c2aabf940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2be5b66-246c-4124-88fb-32a436d756e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\phil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Für Lemmatisierung\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Für reguläre Ausdrücke\n",
    "import re\n",
    "# Für einheitliche Character Representation\n",
    "import unidecode\n",
    "# für Text\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Die nächsten Schritte beinhalten das Laden der Daten und das on-the-fly Preprocessing. Die folgende Funktion aus [1] übertragen um Mengenangaben und Sonderzeichen zu entfernen. Zusätzlich werden die Zutaten Lemmatisiert."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f468ef59-9d63-4a58-a23a-22baeccc2fae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def preprocess(ingredients):\n",
    "    ingredients = ' '.join(ingredients)\n",
    "    ingredients = ingredients.lower() #Convert to lowercase\n",
    "    ingredients = re.sub('[,\\.!?:()\"]', '',ingredients) # remove punctuation marks \n",
    "    ingredients = re.sub('[^a-zA-Z\"]',' ',ingredients) # remove all strings that contain a non-letter\n",
    "    ingredients = ingredients.replace('-', ' ')\n",
    "    words = []\n",
    "    for word in ingredients.split():\n",
    "        word = re.sub(\"[0-9]\",\" \",word) #removing numbers\n",
    "        word = re.sub((r'\\b(oz|ounc|ounce|pound|lb|inch|inches|kg|to)\\b'), ' ', word) # Removing Units\n",
    "        if len(word) <= 2: continue\n",
    "        word = unidecode.unidecode(word)\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        if len(word) > 0: words.append(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e946aae-148d-422d-94a2-3031eac24e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erkennen, ob eine GPU verfügbar ist um den Embeddingvorgang zu beschleunigen\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# kopiert aus der sBERT Dokumentation https://www.sbert.net/examples/applications/computing-embeddings/README.html\n",
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "class IngredientsEmbeddedDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, embedding_mode='mean', bert_model = None, transform=None):\n",
    "        \"\"\"\n",
    "        Argumente:\n",
    "            csv_file (string): Pfad zu der json datei welche die Daten enthält.\n",
    "            embedding_mode (string): Wie die Zutatenliste zu einem Vektor zusammengeführt wird.\n",
    "                                    mean für elementweises Mitteln.\n",
    "                                    combine für Kombinieren zu einem Satz\n",
    "            bert_model (string): encoder model.\n",
    "                                Entweder CookBert (https://github.com/paschistrobel/CookBERT)\n",
    "                                oder None für bert-base-nli-mean-tokens\n",
    "            transform (callable, optional): Eine mögliche transformation\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_json(csv_file)\n",
    "        \n",
    "        classes = self.data_frame['cuisine'].unique()\n",
    "        \n",
    "        self.one_hot_dict = {item : index for index,item in enumerate(classes)}\n",
    "        self.class_counts = self.data_frame.cuisine.value_counts()\n",
    "        \n",
    "        # formula for class weights balanced\n",
    "        self.class_weights = {len(self.data_frame) / (len(classes)*self.class_counts[item]) for index,item in enumerate(classes)}\n",
    "        \n",
    "        self.cached_embeddings = [None] * len(self.data_frame)\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.embedding_mode = embedding_mode\n",
    "        \n",
    "        self.encoder = bert_model\n",
    "        \n",
    "        if self.encoder == 'CookBert': #https://github.com/paschistrobel/CookBERT            \n",
    "            self.CookBERT_tokenizer = BertTokenizerFast.from_pretrained(\"CookBERT-checkpoint\", use_fast=True)\n",
    "            self.CookBERT = BertForMaskedLM.from_pretrained(\"CookBERT-checkpoint\")._modules['bert']            \n",
    "                        \n",
    "            self.CookBERT.to(device)#use gpu for big speedup\n",
    "\n",
    "            self.model = pipeline(task=\"feature-extraction\", model=self.CookBERT, tokenizer=self.CookBERT_tokenizer)\n",
    "        elif self.encoder is None:\n",
    "            self.model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "        else:\n",
    "            print(f\"could not find requested model {bert_model}. falling back to default bert-base-nli-mean-tokens\")\n",
    "            self.model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "            self.encoder = None\n",
    "    \n",
    "    def encode_sentence(self, sentence):\n",
    "        if self.encoder == 'CookBert': #https://github.com/paschistrobel/CookBERT\n",
    "            # Tokenize the sentence.\n",
    "            encoded_inputs = self.CookBERT_tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True).to(device)                      \n",
    "            self.encoded_inputs = encoded_inputs\n",
    "\n",
    "            embedding = self.CookBERT(**encoded_inputs)\n",
    "\n",
    "            mean_pooled_embedding = mean_pooling(embedding, self.encoded_inputs['attention_mask'])                \n",
    "\n",
    "            return mean_pooled_embedding.cpu().detach()\n",
    "        elif self.encoder is None:\n",
    "            return self.model.encode(sentence, device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        ingredients = self.data_frame.iloc[idx].ingredients\n",
    "        \n",
    "        cuisine = self.data_frame.iloc[idx].cuisine\n",
    "        #model.encode(sentences)\n",
    "        embedding = None\n",
    "        \n",
    "        if self.cached_embeddings[idx] is not None:\n",
    "            embedding = self.cached_embeddings[idx]\n",
    "            #print(f\"cache hit for {idx}\")\n",
    "        elif self.embedding_mode is 'mean':\n",
    "            ingredients = preprocess(ingredients)\n",
    "            embeddings = self.encode_sentence(ingredients)\n",
    "            embedding = embeddings.mean(axis = 0)\n",
    "            self.cached_embeddings[idx] = embedding\n",
    "        elif self.embedding_mode is 'combine':\n",
    "            ingredients = preprocess(ingredients)\n",
    "            embedding = self.encode_sentence('The kitchen uses the following ingredients' + ' '.join(ingredients)).squeeze()\n",
    "            #embedding = embeddings.mean(axis = 0)\n",
    "            self.cached_embeddings[idx] = embedding\n",
    "            #print(f\"computed embedding for {idx}\")\n",
    "        #sample = {'embedding': embedding, 'ingredients': ingredients, 'cuisine': cuisine, 'target' : self.one_hot_dict[cuisine]}\n",
    "        sample = {'embedding': embedding,'target' : self.one_hot_dict[cuisine]}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "experiment_index = 0 # in diesem Fall wird das modell bert-base-nli-mean-tokens verwendet, und elementweises mitteln, um die einzelnen embeddings zu vereinen\n",
    "#experiment_index = 1 # in diesem Fall wird das modell bert-base-nli-mean-tokens verwendet, und aus den Zutaten wird ein Satz gebildet welcher embedded wird\n",
    "#experiment_index = 2 # in diesem Fall wird das modell CookBERT verwendet, und elementweises mitteln, um die einzelnen embeddings zu vereinen\n",
    "#experiment_index = 4 # in diesem Fall wird das modell CookBERT verwendet, und aus den Zutaten wird ein Satz gebildet welcher embedded wird"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "if experiment_index == 0:\n",
    "    experiment = \"sBERTMean\"\n",
    "    full_training_set = IngredientsEmbeddedDataset(\"cleaned_common_words.json\", embedding_mode=\"mean\")\n",
    "if experiment_index == 1:\n",
    "    experiment = \"sBERTCombine\"\n",
    "    full_training_set = IngredientsEmbeddedDataset(\"cleaned_common_words.json\", embedding_mode=\"combine\")\n",
    "if experiment_index == 2:\n",
    "    experiment = \"CookBERTMean\"\n",
    "    full_training_set = IngredientsEmbeddedDataset(\"cleaned_common_words.json\", embedding_mode=\"mean\", bert_model=\"CookBert\")\n",
    "if experiment_index == 3:\n",
    "    experiment = \"CookBERTCombine\"\n",
    "    full_training_set = IngredientsEmbeddedDataset(\"cleaned_common_words.json\", embedding_mode=\"combine\", bert_model=\"CookBert\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "52cc0afd-9972-414b-b0cc-9e313248f6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "validation_percentage = 5.0 # Benutze 5% der daten zur Validierung\n",
    "\n",
    "validation_count = int(len(full_training_set) * (validation_percentage / 100))\n",
    "\n",
    "# Aufteilen des Datasets in einen teil für Training und einen für Validierung.\n",
    "train_set, val_set = torch.utils.data.random_split(full_training_set, [len(full_training_set)-validation_count, validation_count])\n",
    "\n",
    "# Die Dataloader helfen die Daten in Batches zu laden, und zu mischen.\n",
    "# Für Training, die Daten mischen\n",
    "training_subset_loader = torch.utils.data.DataLoader(train_set, batch_size=512, shuffle=True)\n",
    "# Für Validierung, die Reihenfolge beibehalten\n",
    "validation_subset_loader = torch.utils.data.DataLoader(val_set, batch_size=512, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ef4865cd-08cf-4f1a-b4d7-be3bcb0d53df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#full_training_set.CookBERT.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "de55e172-dfe5-4145-86f4-90ad74bf4f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'greek': 0, 'southern_us': 1, 'filipino': 2, 'indian': 3, 'jamaican': 4, 'spanish': 5, 'italian': 6, 'mexican': 7, 'chinese': 8, 'thai': 9, 'vietnamese': 10, 'cajun_creole': 11, 'french': 12, 'japanese': 13, 'irish': 14, 'korean': 15, 'moroccan': 16, 'british': 17, 'russian': 18, 'brazilian': 19}\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/36825 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d038e4b176754f44a928862d1f6cd436"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,) 0\n",
      "[-1.92879960e-01  8.42401385e-02  1.47018790e+00 -2.31107816e-01\n",
      "  2.78237134e-01  4.92624849e-01 -1.23385955e-02  6.94262564e-01\n",
      "  8.91929865e-02 -3.23365897e-01 -5.68237424e-01  1.40241235e-01\n",
      "  1.89152732e-01  7.07353234e-01  8.79593432e-01 -4.57285978e-02\n",
      " -7.66454041e-01 -4.66185287e-02  3.11386675e-01 -3.99705410e-01\n",
      "  9.36670750e-02  3.59431326e-01 -3.25278580e-01 -9.04336512e-01\n",
      " -1.12629190e-01 -5.35671175e-01  1.48645312e-01 -1.10582328e+00\n",
      " -1.59317121e-01 -6.19924366e-02  1.85565297e-02  2.04996780e-01\n",
      "  8.43308032e-01 -2.60736234e-02 -3.00469011e-01  3.31688821e-01\n",
      " -3.87700856e-01  1.11413002e-01 -5.80832437e-02  1.85916737e-01\n",
      "  1.23377883e+00  2.77279913e-01  7.52449870e-01  3.74130309e-01\n",
      " -1.95533633e-01 -5.94785772e-02 -7.85126016e-02  5.96378803e-01\n",
      " -4.89116341e-01 -7.67008662e-01 -8.14348310e-02 -8.73381436e-01\n",
      "  9.46973145e-01  4.09914017e-01 -5.18964112e-01  1.49229646e-01\n",
      "  3.51720750e-01 -1.27169266e-01  6.42876267e-01  5.48004091e-01\n",
      "  8.40419680e-02 -2.04407889e-02 -5.51197343e-02  3.42235863e-01\n",
      " -9.26438749e-01  8.12453553e-02  1.01705957e-02  5.26088588e-02\n",
      " -5.00372112e-01  4.22023594e-01  1.00018275e+00 -7.54078388e-01\n",
      " -1.48045659e-01  2.91853964e-01 -4.98501003e-01 -6.71675861e-01\n",
      " -4.06454839e-02  2.52185136e-01  2.95935065e-01  6.54102743e-01\n",
      "  8.60696509e-02 -1.31915808e-02  1.13086164e+00  2.71865312e-04\n",
      " -3.35679770e-01  3.05172890e-01  3.15068096e-01  7.58145928e-01\n",
      " -1.55125129e+00  3.63842309e-01  1.54583678e-01  2.13189125e-01\n",
      "  4.50311929e-01  4.00296267e-04 -4.03803922e-02 -7.60686755e-01\n",
      "  3.14284712e-01 -2.02471972e-01  2.40147218e-01 -2.61243999e-01\n",
      " -9.08667624e-01  4.26379293e-01 -4.85386789e-01  3.44547749e-01\n",
      " -3.61954533e-02  1.03019953e-01 -4.00619537e-01 -3.34752291e-01\n",
      " -4.57349010e-02  3.27266455e-01 -3.05042684e-01  7.45947063e-01\n",
      "  7.57365346e-01 -1.34467423e-01 -7.13182688e-02 -1.04975112e-01\n",
      " -1.01637590e+00  1.74594596e-01 -1.28368497e-01  5.16006827e-01\n",
      "  3.46879750e-01  8.04418743e-01 -1.75603423e-02 -4.11555260e-01\n",
      "  9.71180320e-01 -2.54874378e-01  1.93086728e-01 -5.89904487e-01\n",
      " -6.40944779e-01  6.44140616e-02  1.37103423e-01  7.80620158e-01\n",
      " -1.64490789e-01 -4.15399015e-01 -3.45366538e-01 -1.96656153e-01\n",
      "  2.66673595e-01 -3.70220803e-02  8.95919204e-02 -4.78863865e-01\n",
      " -1.02671063e+00  7.32947350e-01 -4.57801729e-01  3.70948792e-01\n",
      " -5.85429259e-02 -2.42600009e-01  7.16500208e-02  2.44922906e-01\n",
      "  1.15435466e-03  1.46887347e-01 -1.63260922e-01  5.87830424e-01\n",
      " -3.55984718e-01 -2.34852403e-01 -5.46930969e-01 -1.71851203e-01\n",
      " -4.17713165e-01  6.25945270e-01  3.86706024e-01 -2.21795872e-01\n",
      " -7.61594176e-02  5.00440896e-01  1.84553549e-01 -2.07901210e-01\n",
      " -2.01899745e-02 -2.53922611e-01  4.75658700e-02  2.18765333e-01\n",
      "  2.24408314e-01  6.97721168e-02  5.57478249e-01 -8.78290713e-01\n",
      "  3.17045182e-01  1.24030963e-01 -1.98279068e-01  8.45225006e-02\n",
      "  1.27048746e-01  1.08371711e+00  8.56882691e-01 -7.91628584e-02\n",
      " -4.81125377e-02 -5.53504884e-01  1.18936193e+00 -1.12544811e+00\n",
      " -6.55073822e-01 -3.13923538e-01 -4.29545110e-03 -1.97728291e-01\n",
      "  7.42908299e-01  3.88469905e-01 -9.19386864e-01 -7.75024533e-01\n",
      "  2.06944481e-01 -1.00168467e+00  9.29674506e-03  5.22547543e-01\n",
      " -4.09130007e-01 -1.62064925e-01  6.71318248e-02 -5.21699965e-01\n",
      "  7.31568694e-01 -4.56479102e-01  5.56389749e-01  3.27966139e-02\n",
      "  2.93260157e-01  1.05314326e+00 -3.91077131e-01 -9.86742899e-02\n",
      "  7.41462633e-02 -1.87915906e-01 -4.64495838e-01  3.17517787e-01\n",
      "  5.15811324e-01 -1.43015265e-01  1.31865889e-01 -5.77311873e-01\n",
      "  1.01425588e+00 -1.11270435e-01 -1.07528245e+00  7.50977397e-02\n",
      "  2.06787929e-01  1.95900202e-02  1.70567036e-01  6.01442635e-01\n",
      " -6.30205750e-01  5.35122931e-01  3.98019493e-01  3.84481221e-01\n",
      " -1.97828501e-01 -1.68776616e-01  7.39666104e-01 -1.89835712e-01\n",
      " -3.26744825e-01  6.03060603e-01  1.25429884e-01  7.83945844e-02\n",
      "  1.28694430e-01 -3.63327324e-01  5.99698462e-02  2.85021991e-01\n",
      "  6.01167560e-01  4.15231228e-01  1.37565553e-01  8.40479791e-01\n",
      " -1.45896852e+00 -3.83143336e-01  8.67782593e-01 -4.17439975e-02\n",
      " -3.31506193e-01  8.44990313e-01  3.09771150e-01 -9.44677174e-01\n",
      "  3.20143908e-01 -2.94284791e-01  4.79596078e-01 -4.04404819e-01\n",
      " -3.52158993e-01 -4.22393456e-02  5.12603641e-01 -7.74214447e-01\n",
      " -3.97312820e-01 -4.76774752e-01  5.09199919e-03  6.64432198e-02\n",
      " -8.23961720e-02  2.94866651e-01 -5.33877730e-01  9.53954831e-02\n",
      " -9.00417566e-03  2.04212978e-01 -8.78727287e-02 -1.91047356e-01\n",
      " -4.58936244e-01 -4.12091762e-01  1.42517775e-01 -2.65647620e-01\n",
      "  9.58022714e-01 -9.52365994e-03 -1.01851404e+00 -4.32214767e-01\n",
      " -3.90562147e-01  3.51706117e-01 -1.88229167e+00  4.71011698e-01\n",
      " -8.53152573e-01  5.80843799e-02  7.50505805e-01 -8.96706998e-01\n",
      " -5.68242729e-01  4.69673313e-02 -6.41481757e-01  3.43208849e-01\n",
      " -1.38671622e-01 -6.04416668e-01  4.19842936e-02 -5.84301591e-01\n",
      "  1.08023643e-01 -8.63516390e-01 -8.87383580e-01  2.95412123e-01\n",
      "  2.50487149e-01 -3.31398994e-01 -5.02736211e-01  2.29595795e-01\n",
      " -2.21120253e-01 -2.48326167e-01 -1.07108235e-01  4.43728447e-01\n",
      " -3.98394197e-01 -2.27537289e-01 -1.27345312e+00  2.59294748e-01\n",
      "  1.54007688e-01 -2.12324992e-01  3.62467796e-01 -6.15054630e-02\n",
      " -1.07067108e+00  6.26507819e-01  1.59229502e-01 -8.96707058e-01\n",
      " -1.94658250e-01  5.65341055e-01 -4.41206731e-02  5.11972718e-02\n",
      "  5.19403398e-01 -3.52396756e-01 -5.44267058e-01  7.02655911e-01\n",
      " -3.10893208e-01 -1.57193393e-01  1.71687260e-01 -8.08486581e-01\n",
      " -3.15643042e-01  7.52875686e-01 -6.74539432e-02  7.66257167e-01\n",
      " -7.43781686e-01 -7.25666702e-01  2.96372086e-01  3.98558438e-01\n",
      "  2.82278866e-01 -4.63889986e-01  7.59360939e-02 -3.43099564e-01\n",
      "  8.76011074e-01 -3.68250042e-01 -2.57664174e-01 -6.96172059e-01\n",
      "  6.28831923e-01  4.08817321e-01 -2.40531966e-01  1.35690868e-01\n",
      " -3.53138000e-01 -3.52611125e-01 -3.21410626e-01 -5.48666418e-01\n",
      "  7.72161335e-02 -3.08475018e-01 -1.88895345e-01  2.31018990e-01\n",
      " -1.79528579e-01  1.55418739e-01 -5.20107746e-02 -9.70121771e-02\n",
      " -3.98329496e-01 -4.19744365e-02  8.09770167e-01 -9.38347757e-01\n",
      "  1.29510462e+00  4.47838515e-01  4.21019286e-01  3.92572939e-01\n",
      " -1.16138510e-01  2.88193494e-01 -9.50687975e-02  1.78254433e-02\n",
      "  4.17092562e-01 -7.35914409e-01  2.82648116e-01 -2.40667209e-01\n",
      "  7.03060269e-01 -8.27469826e-01  1.48402721e-01 -1.62607089e-01\n",
      " -2.57220101e-02  5.88333979e-02 -3.30141038e-01 -4.86523062e-01\n",
      " -9.04936343e-02 -1.25838280e-01  8.70535195e-01  3.45355839e-01\n",
      " -2.91422993e-01 -2.41377931e-02 -2.65116822e-02 -1.47553623e+00\n",
      "  5.56670427e-02  7.49806941e-01  6.76988810e-02  1.11505672e-01\n",
      "  2.03731805e-01  2.97861069e-01  3.12344611e-01  3.53891790e-01\n",
      "  7.18777537e-01  6.50110781e-01 -2.45474875e-02 -1.90085843e-01\n",
      " -5.28294146e-01 -2.67040998e-01  4.86322433e-01  2.52253205e-01\n",
      " -3.20934117e-01 -2.01826885e-01 -6.62229002e-01  6.93257153e-01\n",
      "  6.93898320e-01 -8.43471646e-01  7.84588933e-01  1.00227344e+00\n",
      " -2.91635096e-01 -3.45764369e-01  1.23139359e-01  4.17684615e-01\n",
      " -3.01020086e-01  4.78335828e-01  9.48894843e-02  4.57282066e-02\n",
      "  3.03913020e-02 -1.81780122e-02  2.80301124e-01  1.71313867e-01\n",
      " -4.76038396e-01  8.65184069e-02  5.61346829e-01 -4.27659094e-01\n",
      " -1.10999644e-01  1.24303125e-01  2.57970132e-02  2.66025998e-02\n",
      "  1.18837081e-01 -5.40145218e-01 -1.18853569e-01  3.79311532e-01\n",
      " -6.40087664e-01 -1.17359743e-01 -2.79229194e-01 -1.20258979e-01\n",
      "  2.66892701e-01 -4.78943676e-01  4.72128808e-01  4.71050501e-01\n",
      " -5.33405356e-02  4.55112398e-01  5.00499666e-01  1.54785261e-01\n",
      " -3.50372612e-01  3.36821020e-01  7.28750288e-01 -2.78196007e-01\n",
      " -4.39340651e-01 -1.88542753e-01 -8.99552464e-01 -1.84285656e-01\n",
      " -6.03123680e-02  6.22160323e-02 -8.48856568e-01  4.19091761e-01\n",
      "  1.93961591e-01  1.54450729e-01  1.57484323e-01  5.78893833e-02\n",
      "  6.73451722e-02 -8.05462524e-02  4.74737138e-01  2.97658503e-01\n",
      " -1.49423921e+00  5.81308842e-01  1.95242956e-01  6.03865683e-01\n",
      " -1.16578057e-01  1.88535079e-01 -3.30828995e-01 -1.14610769e-01\n",
      " -9.18590188e-01  3.22137982e-01  3.61253411e-01  9.50692222e-02\n",
      "  8.45480084e-01 -2.75618643e-01 -2.57466435e-01 -6.23073995e-01\n",
      " -1.03647411e+00 -9.71024811e-01 -5.21340787e-01  4.09294754e-01\n",
      " -6.68179333e-01 -8.32364783e-02 -4.55296040e-03 -1.69747859e-01\n",
      "  3.66745621e-01  3.75615180e-01 -6.26387537e-01 -9.95751500e-01\n",
      "  6.83091506e-02  6.02025926e-01 -1.96162060e-01 -6.54480159e-01\n",
      "  9.35023427e-02  5.98353326e-01 -8.31686020e-01  7.45434403e-01\n",
      " -8.59565586e-02 -6.60551846e-01 -3.95210981e-01  3.26850377e-02\n",
      "  5.46583869e-02  4.84916866e-01  2.89818853e-01 -1.11790657e+00\n",
      " -3.37050378e-01  7.93330818e-02 -7.96710402e-02  4.11559105e-01\n",
      "  9.78732482e-02  1.82175443e-01  6.12412632e-01 -3.58723849e-01\n",
      " -1.22358566e-02 -5.26472628e-01 -1.28723189e-01  2.55262226e-01\n",
      "  7.77365118e-02  8.09329629e-01  1.95259497e-01  9.40178260e-02\n",
      " -5.99381149e-01 -3.86672407e-01  3.78480107e-01  9.28679049e-01\n",
      "  6.52774930e-01  5.78566611e-01  5.78022659e-01 -4.10450459e-01\n",
      " -6.01143301e-01  1.62969068e-01  6.67768195e-02  2.46284857e-01\n",
      "  5.71325362e-01 -8.30238700e-01 -9.19954956e-01  1.84835210e-01\n",
      "  3.09778839e-01 -1.51215687e-01 -9.02830213e-02 -6.41056478e-01\n",
      " -5.18534720e-01  6.84384704e-01 -1.28524616e-01 -6.73522770e-01\n",
      "  2.97551095e-01 -1.75802439e-01 -1.32234827e-01 -3.73344570e-01\n",
      " -5.09377062e-01 -1.09132528e-01 -2.69219071e-01 -3.58066149e-02\n",
      " -4.67229158e-01  2.00434670e-01  5.34124970e-01 -4.13963586e-01\n",
      "  7.40608871e-01  4.45565075e-01 -2.43129700e-01 -7.56598949e-01\n",
      "  5.09047568e-01 -1.56527445e-01 -9.19744372e-01 -3.08168381e-01\n",
      "  7.38307655e-01  9.14826214e-01  2.94436276e-01 -3.39159817e-01\n",
      " -8.98320302e-02 -2.49223307e-01  4.53603476e-01 -1.31342030e+00\n",
      "  1.96900330e-02  7.79416561e-01  3.57621133e-01 -1.20049700e-01\n",
      " -2.91700661e-01 -1.02915621e+00 -1.43058643e-01  5.09376489e-02\n",
      " -5.41245878e-01  7.80034482e-01 -6.80653006e-02  4.64157134e-01\n",
      " -2.97221899e-01 -2.03812167e-01 -8.30015361e-01  9.14991260e-01\n",
      " -7.12694585e-01 -2.01580554e-01 -5.58490872e-01 -3.59075367e-01\n",
      " -2.45182917e-01  4.61954206e-01  3.05108488e-01 -8.21130574e-01\n",
      " -5.80491163e-02  8.34201947e-02 -2.32306853e-01 -6.54006481e-01\n",
      " -1.60318136e-01  4.35194932e-02  1.13218725e+00  2.24754170e-01\n",
      " -5.78527391e-01  1.22154839e-01  2.64669098e-02  4.05301303e-01\n",
      " -2.56023347e-01 -7.09021091e-01  2.63590634e-01 -1.23972110e-01\n",
      "  1.08201697e-01  1.13023110e-01  6.33762658e-01  1.36308894e-01\n",
      "  4.72196817e-01  3.02701950e-01 -6.45342231e-01  7.35954583e-01\n",
      "  2.13953495e-01 -5.25080636e-02  2.06836671e-01 -3.77652258e-01\n",
      " -2.71266192e-01 -9.07431722e-01  5.24416566e-01 -3.65757853e-01\n",
      "  2.29975745e-01 -3.10352594e-01 -3.01772892e-01  4.90076274e-01\n",
      "  4.85389262e-01  5.93513787e-01 -6.22870736e-02 -9.28007245e-01\n",
      " -6.58454418e-01  1.17467798e-01  4.04158905e-02 -2.82361835e-01\n",
      " -2.27754593e-01  3.26968372e-01 -2.63338000e-01 -7.59807646e-01\n",
      "  4.65327919e-01 -1.81485519e-01 -2.48859540e-01  3.62262726e-02\n",
      "  2.89686203e-01 -1.60777226e-01 -3.80310386e-01  4.85610455e-01\n",
      " -2.78940588e-01 -3.08304489e-01  5.23425639e-01 -1.57703117e-01\n",
      "  3.75595868e-01 -6.61853850e-01 -7.33060658e-01 -6.08069360e-01\n",
      "  2.97371477e-01  2.12568089e-01  1.83825225e-01 -3.64305288e-01\n",
      "  9.43282187e-01  5.38304806e-01  5.91863096e-02  4.95305210e-01\n",
      "  4.65180203e-02  1.19304933e-01  3.00269604e-01  2.53019035e-01\n",
      " -8.79246891e-01 -1.14740863e-01  7.89849102e-01  3.73830467e-01\n",
      " -7.11760074e-02 -1.18220592e+00 -1.28539607e-01  5.56879997e-01\n",
      " -6.57843873e-02 -6.00846469e-01 -3.19375128e-01 -2.18999043e-01\n",
      " -6.70948982e-01 -3.36730450e-01 -9.63305891e-01  1.50358990e-01\n",
      " -2.34604821e-01 -5.15747249e-01 -6.15983129e-01 -3.72316808e-01\n",
      " -2.80746996e-01  5.02779067e-01  2.79218286e-01  6.06163204e-01\n",
      " -7.79092908e-01  1.09261596e+00  3.24744545e-02  1.29988003e+00\n",
      "  2.33686075e-01  4.30436641e-01  3.89201939e-01 -9.53835964e-01\n",
      " -6.28787458e-01  1.92081630e-02 -6.31433129e-01  3.50540012e-01\n",
      " -7.30821908e-01 -1.49469212e-01  3.00543070e-01 -5.63142933e-02\n",
      " -7.88063705e-01 -5.85025251e-01 -2.86931217e-01  5.42776048e-01\n",
      " -5.19935012e-01  6.09287441e-01  6.30139351e-01  3.26546997e-01\n",
      "  4.63047594e-01 -2.17292998e-02 -4.27796394e-01  8.76754582e-01\n",
      " -4.72786516e-01 -3.23291868e-01 -6.79479307e-03 -1.16718337e-01\n",
      " -3.95055674e-02 -3.40826720e-01 -3.19890082e-01  4.30811614e-01\n",
      " -6.25574648e-01 -2.10199013e-01 -4.46480513e-02  3.48083898e-02\n",
      " -3.95942964e-02 -4.19829905e-01 -1.63805753e-01  4.91058975e-01\n",
      " -3.60151790e-02  2.09484175e-01  4.56480592e-01 -5.80771804e-01\n",
      "  4.03075427e-01  3.99149895e-01 -6.61912039e-02 -5.13066471e-01\n",
      "  1.08641274e-01  2.74082094e-01  4.51095849e-01  1.45319745e-01]\n"
     ]
    }
   ],
   "source": [
    "print(full_training_set.one_hot_dict)\n",
    "\n",
    "printed_embeddings = 1\n",
    "for i, sample in enumerate(tqdm(full_training_set)):\n",
    "\n",
    "    #print(i, sample['cuisine'],sample['ingredients'])\n",
    "    print(sample['embedding'].shape, sample['target'])\n",
    "    print(sample['embedding'])\n",
    "    if i == printed_embeddings -1:        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "289f9c9e-69a5-497a-8a8d-6a2a1b1658dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetDeeper2(\n",
      "  (fc1): Linear(in_features=768, out_features=1024, bias=True)\n",
      "  (dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=1024, out_features=800, bias=True)\n",
      "  (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  (fc3): Linear(in_features=800, out_features=512, bias=True)\n",
      "  (dropout3): Dropout(p=0.2, inplace=False)\n",
      "  (fc4): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (dropout4): Dropout(p=0.2, inplace=False)\n",
      "  (fc5): Linear(in_features=256, out_features=20, bias=True)\n",
      "  (log_soft_max): LogSoftmax(dim=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NetDeeper2(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NetDeeper2, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 1024)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(1024, 800)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(800, 512)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.fc4 = nn.Linear(512, 256)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        self.fc5 = nn.Linear(256, 20)\n",
    "        self.log_soft_max = torch.nn.LogSoftmax()    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(x)\n",
    "        x = F.leaky_relu(self.fc1(x)) #leaky relu because of negative input values\n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = F.leaky_relu(self.fc4(x))\n",
    "        x = self.fc5(x)        \n",
    "        return self.log_soft_max(x)\n",
    "\n",
    "\n",
    "\n",
    "model = NetDeeper2()\n",
    "\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "23774742-1068-4c4d-a208-41f7c5cb3467",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.NLLLoss(weight=torch.FloatTensor(list(full_training_set.class_weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d106300f-5c6d-4905-8b24-b70ab658f15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "973e978b-77dd-434a-8c01-f79b92260648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer, epochs):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_subset_loader) instead of\n",
    "    # iter(training_subset_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    loop = tqdm(training_subset_loader,desc=f\"Epoch [{epoch_index}/{epochs}]\")\n",
    "    for i, data in enumerate(loop):                \n",
    "        \n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data['embedding'],data['target']\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)        \n",
    "        \n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        loss_item = loss.item()\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_subset_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "        loop.set_description(f\"Epoch [{epoch_index}/{epochs}]\")\n",
    "        loop.set_postfix(loss=loss_item)\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "840341af-3c93-41fc-8f79-e56d5b554459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch [0/100]:   0%|          | 0/69 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38d1561ece184770b9f52f859f54b25d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "os.makedirs(\"Models\")\n",
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter(f'runs/{experiment}_{timestamp}')\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer, EPOCHS)\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_subset_loader):\n",
    "            vinputs, vlabels = vdata['embedding'],vdata['target']\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "            correct += (np.argmax(voutputs, axis=1) == vlabels).sum()  #enable if batching works\n",
    "\n",
    "    \n",
    "    accuracy = correct / (len(validation_subset_loader)*validation_subset_loader.batch_size)\n",
    "    print(\"Accuracy = {}\".format(accuracy))\n",
    "    \n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "\n",
    "    #writer.add_scalars('Accuracy',\n",
    "    #                { 'Training' : 0, 'Validation' : accuracy },\n",
    "    #                epoch_number + 1)\n",
    "    \n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = f'Models/{experiment}_{timestamp}_{epoch_number}'\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
